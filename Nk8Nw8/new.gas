.data
.align 5
__rc:
  .byte 1, 0, 0, 0
  .byte 1, 0, 0, 0
  .byte 1, 0, 0, 0
  .byte 1, 0, 0, 0

__shuf_2:
  .byte 13, 14, 15, 12
  .byte 13, 14, 15, 12
  .byte 13, 14, 15, 12
  .byte 13, 14, 15, 12

__ctrswap:
  .byte 0, 1, 2, 3
  .byte 4, 5, 6, 7
  .byte 8, 9, 10, 11
  .byte 15, 14, 13, 12
  .byte 0, 1, 2, 3
  .byte 4, 5, 6, 7
  .byte 8, 9, 10, 11
  .byte 15, 14, 13, 12

__zeroone:
__zero:
  .quad 0
  .quad 0
__onetwo:
__one:
  .byte 0, 0, 0, 0
  .byte 0, 0, 0, 0
  .byte 0, 0, 0, 0
  .byte 1, 0, 0, 0
__twotwo:
__two:
  .byte 0, 0, 0, 0
  .byte 0, 0, 0, 0
  .byte 0, 0, 0, 0
  .byte 2, 0, 0, 0
  .byte 0, 0, 0, 0
  .byte 0, 0, 0, 0
  .byte 0, 0, 0, 0
  .byte 2, 0, 0, 0




.macro linearmix key, new
  VPSLLDQ $4, \key, %xmm1
  VPXOR %xmm1, \key, \key
  VPSLLDQ $4, %xmm1, %xmm1
  VPXOR %xmm1, \key, \key
  VPSLLDQ $4, %xmm1, %xmm1
  VPXOR %xmm1, \key, \key
  VPXOR %xmm0, \key, \key
.endm

.macro do const
  MOV \const, %rax
  VMOVQ %rax, %xmm4
  PSHUFD $0, %xmm4, %xmm4
  VPSHUFB __shuf_2(%rip), %xmm3, %xmm0
  VAESENCLAST %xmm4, %xmm0, %xmm0

  linearmix %xmm2, %xmm0
  VMOVDQU %xmm2, 16*2(%rdi)

  VPSHUFD $0xff, %xmm2, %xmm0
  VAESENCLAST %xmm5, %xmm0, %xmm0

  linearmix %xmm3, %xmm0
  VMOVDQU %xmm3, 16*3(%rdi)
  ADDQ $32, %rdi
.endmacro

.text

.align 5
.globl _Rijndael_k32b32_expandkey
_Rijndael_k32b32_expandkey:
  VZEROUPPER

  VMOVDQU 0(%rsi), %xmm2
  VMOVDQU 16(%rsi), %xmm3

  VMOVDQU %xmm2, 0(%rdi)
  VMOVDQU %xmm3, 16(%rdi)

  VMOVDQU __rc(%rip), %xmm4

  VPXOR %xmm5, %xmm5, %xmm5
  do $0x01
  do $0x02
  do $0x04
  do $0x08
  do $0x10
  do $0x20
  do $0x40
  do $0x80

  do $0x1b
  do $0x36
  do $0x6c
  do $0xd8
  do $0xab
  do $0x4d


  VZEROALL
  XOR %rdi, %rdi
  XOR %rsi, %rsi
  RET
